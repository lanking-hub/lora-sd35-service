import os
import torch
import uuid
from datetime import datetime
from diffusers import StableDiffusion3Pipeline
import warnings
from typing import Tuple, Dict, Any
from utils import upload_file_to_oss, get_oss_config_from_env, generate_title_qwen

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings("ignore")
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
# ç¦ç”¨ tokenizer å¹¶è¡Œè½¬æ¢ï¼Œé¿å…å¡ä½
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
# è®¾ç½®ä¸´æ—¶æ–‡ä»¶ç›®å½•ï¼ˆè·¨å¹³å°å…¼å®¹ï¼‰
if os.name == 'nt':  # Windows
    os.environ['TMP'] = 'D:\\temp'
    os.environ['TEMP'] = 'D:\\temp'
else:  # Linux/Docker
    os.environ['TMP'] = '/tmp'
    os.environ['TEMP'] = '/tmp'

# ============= å…¨å±€é…ç½® =============
# è®¾ç½® Hugging Face é•œåƒï¼ˆå›½å†…åŠ é€Ÿï¼‰
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# OSS æŒ‚è½½é…ç½®ï¼ˆå‡½æ•°è®¡ç®—éƒ¨ç½²æ—¶ä½¿ç”¨ï¼‰
OSS_MOUNT_POINT = os.getenv("OSS_MOUNT_POINT", "/mnt/oss")  # OSS æŒ‚è½½ç‚¹
OSS_MODEL_PATH = os.path.join(OSS_MOUNT_POINT, "models", "sd35-medium")  # OSS ä¸Šçš„æ¨¡å‹è·¯å¾„

# åŸºç¡€æ¨¡å‹è·¯å¾„é…ç½®
# ä¼˜å…ˆçº§ï¼šç¯å¢ƒå˜é‡ > OSS æŒ‚è½½è·¯å¾„ > Hugging Face Hub
# æ³¨æ„ï¼šä¸å†ä½¿ç”¨ /tmp ç¼“å­˜ï¼Œå› ä¸º 44GB æ¨¡å‹è¶…è¿‡ä¸´æ—¶ç©ºé—´é™åˆ¶ï¼ˆ10GBï¼‰
if os.getenv("BASE_MODEL_PATH"):
    # éƒ¨ç½²æ—¶é€šè¿‡ç¯å¢ƒå˜é‡æŒ‡å®šï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    BASE_MODEL_PATH = os.getenv("BASE_MODEL_PATH")
    print(f"âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„æ¨¡å‹è·¯å¾„: {BASE_MODEL_PATH}")
elif os.path.exists(OSS_MODEL_PATH):
    # å‡½æ•°è®¡ç®—ç¯å¢ƒï¼šç›´æ¥ä½¿ç”¨ OSS æŒ‚è½½è·¯å¾„
    BASE_MODEL_PATH = OSS_MODEL_PATH
    print(f"âœ… æ£€æµ‹åˆ° OSS æŒ‚è½½ï¼Œä½¿ç”¨ OSS æ¨¡å‹è·¯å¾„: {BASE_MODEL_PATH}")
else:
    # æœ¬åœ°å¼€å‘ï¼šä½¿ç”¨ Hugging Face Hub
    BASE_MODEL_PATH = "stabilityai/stable-diffusion-3.5-medium"
    print(f"âš ï¸  æœªæ£€æµ‹åˆ° OSS æŒ‚è½½ï¼Œå°†ä½¿ç”¨ HuggingFace Hub: {BASE_MODEL_PATH}")

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))

# å“ç‰Œåˆ° LoRA æ–‡ä»¶çš„æ˜ å°„
BRAND_LORA_MAP = {
    "zara": os.path.join(PROJECT_ROOT, "zara", "pytorch_lora_weights.safetensors"),
    "hoc": os.path.join(PROJECT_ROOT, "hoc", "pytorch_lora_weights.safetensors"),
    "cos": os.path.join(PROJECT_ROOT, "cos", "pytorch_lora_weights.safetensors"),
    "rl": os.path.join(PROJECT_ROOT, "rl", "pytorch_lora_weights.safetensors"),
    "lulu": os.path.join(PROJECT_ROOT, "lulu", "pytorch_lora_weights.safetensors"),
}

# è®¾å¤‡é…ç½® - è‡ªåŠ¨é€‰æ‹© GPU æˆ– CPU
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# å…¨å±€ pipeline å¯¹è±¡ (å¤ç”¨ä»¥å‡å°‘åŠ è½½æ—¶é—´)
_pipe: Any = None


def load_pipeline():
    """å»¶è¿ŸåŠ è½½ pipeline,åªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶åŠ è½½

    Returns:
        StableDiffusion3Pipeline: åŠ è½½å¥½çš„ pipeline
    """
    global _pipe

    if _pipe is not None:
        return _pipe

    print(f"ğŸ“¦ æ­£åœ¨åŠ è½½ SD3.5 æ¨¡å‹...")
    print(f"   æ¨¡å‹è·¯å¾„: {BASE_MODEL_PATH}")
    print(f"   è®¾å¤‡: {DEVICE}")

    try:
        print("   æ­£åœ¨åŠ è½½æ¨¡å‹ç»„ä»¶ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")

        # æ ¹æ®è®¾å¤‡é€‰æ‹©æ•°æ®ç±»å‹
        if DEVICE == "cuda":
            print("   âœ… ä½¿ç”¨ GPU æ¨¡å¼ï¼ˆå¿«é€Ÿï¼‰")
            dtype = torch.float16
        else:
            print("   âš ï¸  ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆè¾ƒæ…¢ï¼Œçº¦ 30 åˆ†é’Ÿ/å¼ ï¼‰")
            dtype = torch.float32

        # åˆ¤æ–­æ˜¯å¦ä»æœ¬åœ°è·¯å¾„åŠ è½½ï¼ˆOSS æŒ‚è½½æˆ–ç¯å¢ƒå˜é‡æŒ‡å®šï¼‰
        is_local_path = (
            os.path.exists(BASE_MODEL_PATH) or  # è·¯å¾„å­˜åœ¨
            BASE_MODEL_PATH.startswith("/") or  # Linux ç»å¯¹è·¯å¾„
            BASE_MODEL_PATH.startswith("./") or  # ç›¸å¯¹è·¯å¾„
            BASE_MODEL_PATH.startswith("../") or
            (len(BASE_MODEL_PATH) > 1 and BASE_MODEL_PATH[1] == ':')  # Windows è·¯å¾„ (C:\, E:\, ...)
        )

        load_kwargs = {
            "torch_dtype": dtype,
            "use_safetensors": True,
            "low_cpu_mem_usage": True,
        }

        # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼Œæ·»åŠ  local_files_only=True é¿å…è®¿é—® HuggingFace
        if is_local_path:
            load_kwargs["local_files_only"] = True
            print(f"   ğŸ”’ ä½¿ç”¨æœ¬åœ°æ–‡ä»¶æ¨¡å¼ (local_files_only=True)")

        _pipe = StableDiffusion3Pipeline.from_pretrained(
            BASE_MODEL_PATH,
            **load_kwargs
        ).to(DEVICE)

        # å¯ç”¨å†…å­˜ä¼˜åŒ–
        _pipe.enable_attention_slicing()

        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

    except Exception as e:
        import traceback
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print(f"\nè¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()
        raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")

    return _pipe


def validate_request(event: Dict[str, Any]) -> Tuple[str, str]:
    """éªŒè¯è¯·æ±‚æ•°æ®

    Args:
        event: è¯·æ±‚æ•°æ®å­—å…¸

    Returns:
        (brand, prompt): å“ç‰Œåç§°å’Œæç¤ºè¯

    Raises:
        ValueError: è¯·æ±‚æ•°æ®æ— æ•ˆæ—¶æŠ›å‡ºå¼‚å¸¸
    """
    brand = event.get("brand", "").lower()
    prompt = event.get("prompt", "")

    # éªŒè¯å“ç‰Œ
    if not brand:
        raise ValueError("ç¼ºå°‘ 'brand' å‚æ•°")

    if brand not in BRAND_LORA_MAP:
        supported_brands = list(BRAND_LORA_MAP.keys())
        raise ValueError(
            f"ä¸æ”¯æŒçš„å“ç‰Œ: '{brand}'ã€‚"
            f"æ”¯æŒçš„å“ç‰Œ: {supported_brands}"
        )

    # éªŒè¯ prompt
    if not prompt:
        raise ValueError("ç¼ºå°‘ 'prompt' å‚æ•°")

    if not isinstance(prompt, str):
        raise ValueError("'prompt' å¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹")

    if len(prompt) > 2000:
        raise ValueError("'prompt' é•¿åº¦ä¸èƒ½è¶…è¿‡ 2000 å­—ç¬¦")

    return brand, prompt


def load_lora_weights(pipe, lora_path: str) -> None:
    """åŠ è½½ LoRA æƒé‡

    Args:
        pipe: Stable Diffusion pipeline
        lora_path: LoRA æƒé‡æ–‡ä»¶è·¯å¾„

    Raises:
        RuntimeError: LoRA åŠ è½½å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    if not os.path.exists(lora_path):
        raise FileNotFoundError(f"LoRA æ–‡ä»¶ä¸å­˜åœ¨: {lora_path}")

    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½ LoRA æƒé‡...")
    print(f"   LoRA è·¯å¾„: {lora_path}")

    try:
        # å°è¯•å¤šç§åŠ è½½æ–¹æ³•
        try:
            pipe.load_lora_weights(
                lora_path,
                adapter_name="brand_lora",
                weight_name="pytorch_lora_weights.safetensors"
            )
            print("âœ… LoRA æƒé‡åŠ è½½æˆåŠŸ (æ–¹æ³•1)")
        except Exception as e1:
            print(f"   æ–¹æ³•1 å¤±è´¥: {e1}")
            try:
                pipe.load_lora_weights(lora_path)
                print("âœ… LoRA æƒé‡åŠ è½½æˆåŠŸ (æ–¹æ³•2)")
            except Exception as e2:
                print(f"   æ–¹æ³•2 å¤±è´¥: {e2}")
                raise RuntimeError(
                    f"LoRA æƒé‡åŠ è½½å¤±è´¥ã€‚è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚"
                )

    except Exception as e:
        raise RuntimeError(f"LoRA æƒé‡åŠ è½½å¤±è´¥: {str(e)}")


def generate_image(pipe, prompt: str, seed: int = 42) -> Any:
    """ç”Ÿæˆå›¾åƒ

    Args:
        pipe: Stable Diffusion pipeline
        prompt: æç¤ºè¯
        seed: éšæœºç§å­

    Returns:
        ç”Ÿæˆçš„ PIL Image å¯¹è±¡

    Raises:
        RuntimeError: å›¾åƒç”Ÿæˆå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    print(f"ğŸ¨ æ­£åœ¨ç”Ÿæˆå›¾åƒ...")
    print(f"   æç¤ºè¯: {prompt[:100]}{'...' if len(prompt) > 100 else ''}")

    try:
        generator = torch.Generator(DEVICE).manual_seed(seed)

        image = pipe(
            prompt=prompt,
            num_inference_steps=30,
            guidance_scale=6.0,
            height=896,
            width=896,
            generator=generator,
        ).images[0]

        print(f"âœ… å›¾åƒç”ŸæˆæˆåŠŸ (å°ºå¯¸: {image.size})")
        return image

    except Exception as e:
        raise RuntimeError(f"å›¾åƒç”Ÿæˆå¤±è´¥: {str(e)}")


def save_and_upload_image(image, brand: str) -> str:
    """ä¿å­˜å›¾åƒåˆ°æœ¬åœ°å¹¶ä¸Šä¼ åˆ°é˜¿é‡Œäº‘ OSS

    Args:
        image: PIL Image å¯¹è±¡
        brand: å“ç‰Œåç§°

    Returns:
        str: OSS ç­¾å URL

    Raises:
        RuntimeError: ä¿å­˜æˆ–ä¸Šä¼ å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå‡½æ•°è®¡ç®—ä½¿ç”¨ /tmpï¼Œæœ¬åœ°å¼€å‘ä½¿ç”¨å½“å‰ç›®å½•ï¼‰
    if os.path.exists("/tmp"):  # å‡½æ•°è®¡ç®—ç¯å¢ƒ
        output_dir = "/tmp/images"
    else:  # æœ¬åœ°å¼€å‘ç¯å¢ƒ
        output_dir = os.path.join(PROJECT_ROOT, "lora_outputs")
    os.makedirs(output_dir, exist_ok=True)

    # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    unique_id = uuid.uuid4().hex[:8]
    filename = f"{brand}_{timestamp}_{unique_id}.png"
    file_path = os.path.join(output_dir, filename)

    # ä¿å­˜åˆ°æœ¬åœ°
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜å›¾åƒ...")
    try:
        image.save(file_path)
        print(f"âœ… å›¾åƒå·²ä¿å­˜: {file_path}")
    except Exception as e:
        raise RuntimeError(f"å›¾åƒä¿å­˜å¤±è´¥: {str(e)}")

    # ä¸Šä¼ åˆ° OSS
    print(f"â˜ï¸  æ­£åœ¨ä¸Šä¼ åˆ°é˜¿é‡Œäº‘ OSS...")
    try:
        oss_config = get_oss_config_from_env()
        image_url = upload_file_to_oss(
            file_path=file_path,
            oss_config=oss_config,
            object_key_prefix="lora_images",
            delete_after_upload=True  # ä¸Šä¼ ååˆ é™¤æœ¬åœ°æ–‡ä»¶
        )

        if not image_url:
            raise RuntimeError("OSS ä¸Šä¼ è¿”å›ç©º URL")

        print(f"âœ… ä¸Šä¼ æˆåŠŸ: {image_url}")
        return image_url

    except Exception as e:
        # ä¸Šä¼ å¤±è´¥æ—¶ä¿ç•™æœ¬åœ°æ–‡ä»¶
        print(f"âš ï¸  OSS ä¸Šä¼ å¤±è´¥: {e}")
        print(f"   æœ¬åœ°æ–‡ä»¶ä¿ç•™: {file_path}")
        raise RuntimeError(f"å›¾åƒä¸Šä¼  OSS å¤±è´¥: {str(e)}")


def main(event: Dict[str, Any]) -> Tuple[bool, Any]:
    """ä¸»å‡½æ•°:æ¥æ”¶ brand å’Œ prompt,ç”Ÿæˆå›¾åƒå¹¶è¿”å› URL å’Œæ ‡é¢˜

    Args:
        event: è¯·æ±‚æ•°æ®å­—å…¸,åŒ…å«:
            - brand: å“ç‰Œåç§° (zara/hoc/cos/rl/lulu)
            - prompt: å›¾åƒç”Ÿæˆæç¤ºè¯

    Returns:
        (success, result): tuple
            - success: bool, æ˜¯å¦æˆåŠŸ
            - result: æˆåŠŸæ—¶ä¸º dict {"url": str, "title": str}
                     å¤±è´¥æ—¶ä¸º str (é”™è¯¯ä¿¡æ¯)

    ç¤ºä¾‹:
        >>> event = {"brand": "zara", "prompt": "A white dress"}
        >>> success, result = main(event)
        >>> if success:
        ...     print(result["url"], result["title"])
    """
    print(f"\n{'='*60}")
    print(f"æ”¶åˆ°æ–°çš„å›¾åƒç”Ÿæˆè¯·æ±‚")
    print(f"{'='*60}\n")

    try:
        # 1. éªŒè¯è¯·æ±‚æ•°æ®
        brand, prompt = validate_request(event)
        print(f"ğŸ“‹ è¯·æ±‚å‚æ•°:")
        print(f"   å“ç‰Œ: {brand}")
        print(f"   æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦\n")

        # 2. è·å– LoRA è·¯å¾„
        lora_path = BRAND_LORA_MAP[brand]
        print(f"ğŸ” LoRA é…ç½®:")
        print(f"   è·¯å¾„: {lora_path}")
        print(f"   æ–‡ä»¶å­˜åœ¨: {os.path.exists(lora_path)}\n")

        # 3. åŠ è½½ pipeline
        pipe = load_pipeline()

        # 4. åŠ è½½ LoRA æƒé‡
        load_lora_weights(pipe, lora_path)

        # 5. å¤„ç†æç¤ºè¯ï¼ˆç¿»è¯‘+ç”Ÿæˆæ ‡é¢˜ï¼‰
        print(f"\næ­£åœ¨å¤„ç†æç¤ºè¯...")
        english_prompt, title = generate_title_qwen(prompt)
        print(f"   åŸæ–‡: {prompt[:60]}{'...' if len(prompt) > 60 else ''}")
        print(f"   è‹±æ–‡: {english_prompt}")
        print(f"   æ ‡é¢˜: {title}\n")

        # 6. ç”Ÿæˆå›¾åƒï¼ˆä½¿ç”¨ç¿»è¯‘åçš„è‹±æ–‡æç¤ºè¯ï¼‰
        image = generate_image(pipe, english_prompt, seed=42)

        # 6. ä¿å­˜å¹¶ä¸Šä¼ 
        image_url = save_and_upload_image(image, brand)

        print(f"\n{'='*60}")
        print(f"âœ… å›¾åƒç”Ÿæˆå®Œæˆ")
        print(f"{'='*60}\n")

        return True, {"url": image_url, "title": title}

    except Exception as e:
        print(f"\n{'='*60}")
        print(f"âŒ å¤„ç†å¤±è´¥: {str(e)}")
        print(f"{'='*60}\n")
        return False, str(e)


# ============= æœ¬åœ°æµ‹è¯• =============
if __name__ == "__main__":
    # æµ‹è¯•äº‹ä»¶
    test_event = {
        "brand": "zara",
        "prompt": "Off-white top, chiffon, with a small amount of matching color embroidery, sleeveless, flowing"
    }

    # è¿è¡Œæµ‹è¯•
    success, result = main(test_event)

    if success:
        print(f"\nâœ… æµ‹è¯•æˆåŠŸ!")
        print(f"å›¾åƒ URL: {result['url']}")
        print(f"æ ‡é¢˜: {result['title']}")
    else:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥!")
        print(f"é”™è¯¯ä¿¡æ¯: {result}")
